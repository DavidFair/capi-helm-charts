{{- if and .Values.cni.enabled (eq .Values.cni.type "calico") }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "cluster-addons.componentName" (list . "cni-calico") }}-{{ .Release.Revision }}
  labels: {{ include "cluster-addons.componentLabels" (list . "cni-calico") | nindent 4 }}
spec:
  # Keep trying for a decent amount of time before failing
  backoffLimit: 1000
  # Keep succeeded jobs for 5m after finishing
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      labels: {{ include "cluster-addons.componentSelectorLabels" (list . "cni-calico") | nindent 8 }}
    spec:
      serviceAccountName: {{ include "cluster-addons.componentName" (list . "deployer") }}
      restartPolicy: OnFailure
      containers:
        - name: cni-calico
          image: {{ printf "%s:%s" .Values.jobImage.repository (default .Chart.AppVersion .Values.jobImage.tag) }}
          imagePullPolicy: {{ .Values.jobImage.pullPolicy }}
          args:
            - /bin/sh
            - -c
            - |
                set -ex
                kubectl apply -f {{ tpl .Values.cni.calico.manifestURL . }}
                kubectl -n kube-system rollout status daemonset/calico-node
      # This installs a primary CNI, so must be able to run on unready/uninitialised nodes
      tolerations: {{ .Values.bootstrapTolerations | toYaml | nindent 8 }}
      # Because it provides the container networking, it must use host networking
      hostNetwork: true
{{- end }}
