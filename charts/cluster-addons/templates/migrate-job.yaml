{{/*
  Job that cleans up artifacts from the previous job-based addon installation
  in preparation for creating addon objects.

  We only produce the job if jobs from a previous installation exist.
*/}}
{{- $clusterName := include "cluster-addons.clusterName" . }}
{{- $exists := false }}
{{- range $job := (lookup "batch/v1" "Job" .Release.Namespace "").items }}
  {{-
    $exists = or
      $exists
      (and
        (index $job.metadata.labels "app.kubernetes.io/name" | default "" | eq "addons")
        (index $job.metadata.labels "app.kubernetes.io/instance" | default "" | eq $clusterName)
      )
  }}
{{- end }}
{{- if $exists }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "cluster-addons.componentName" (list . "addons-migrate") }}
  labels: {{ include "cluster-addons.componentLabels" (list . "addons-migrate") | nindent 4 }}
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  backoffLimit: {{ .Values.hooks.backoffLimit }}
  activeDeadlineSeconds: {{ .Values.hooks.activeDeadlineSeconds }}
  template:
    metadata:
      labels: {{ include "cluster-addons.componentSelectorLabels" (list . "addons-migrate") | nindent 8 }}
    spec:
      {{- with .Values.hooks.imagePullSecrets }}
      imagePullSecrets: {{ toYaml . | nindent 8 }}
      {{- end }}
      securityContext: {{ toYaml .Values.hooks.podSecurityContext | nindent 8 }}
      restartPolicy: OnFailure
      containers:
        - name: addons-migrate
          image: {{
            printf "%s:%s"
              .Values.hooks.image.repository
              (default .Chart.AppVersion .Values.hooks.image.tag)
          }}
          imagePullPolicy: {{ .Values.hooks.image.pullPolicy }}
          securityContext: {{ toYaml .Values.hooks.securityContext | nindent 12 }}
          args:
            - /bin/bash
            - -c
            - |
                set -ex
                test -f "$KUBECONFIG" || exit 0
                kubectl version || exit 0

                # Remove all the old kustomize releases where possible
                helm status -n kustomize-releases ccm-openstack && \
                  helm delete -n kustomize-releases ccm-openstack
                helm status -n kustomize-releases metrics-server && \
                  helm delete -n kustomize-releases metrics-server

                # The csi-cinder kustomize release contains the Cinder storage class, which we cannot delete
                # if there are volumes associated with it
                # Instead, we move the release to the new namespace, move the storage class into a separate
                # release and annotate the storage class so that it doesn't get removed by the Helm upgrade
                if helm status -n kustomize-releases csi-cinder; then
                  helm-move csi-cinder kustomize-releases {{ .Values.openstack.targetNamespace }}
                  helm-adopt \
                    csi-cinder-storageclass \
                    {{ .Values.openstack.targetNamespace }} \
                    storageclass/{{ .Values.openstack.csiCinder.storageClass.name }}
                  kubectl annotate \
                    storageclass/{{ .Values.openstack.csiCinder.storageClass.name }} \
                    "helm.sh/resource-policy=keep"
                fi

                # Adopt resources previously created in post-install scripts into the relevant Helm releases
                helm-adopt \
                  cni-calico \
                  {{ .Values.cni.calico.release.namespace }} \
                  installation/default
                helm-adopt \
                  kube-prometheus-stack-dashboards \
                  {{ .Values.monitoring.kubePrometheusStack.release.namespace }} \
                  configmap/nvidia-dcgm-exporter-dashboard \
                  --namespace {{ .Values.monitoring.kubePrometheusStack.release.namespace }}
                helm-adopt \
                  loki-stack-dashboards \
                  {{ .Values.monitoring.kubePrometheusStack.release.namespace }} \
                  configmap/loki-stack-grafana-datasource \
                  --namespace {{ .Values.monitoring.kubePrometheusStack.release.namespace }}
                helm-adopt \
                  loki-stack-dashboards \
                  {{ .Values.monitoring.kubePrometheusStack.release.namespace }} \
                  configmap/loki-stack-grafana-dashboard \
                  --namespace {{ .Values.monitoring.kubePrometheusStack.release.namespace }}

                # With the version bump to 40.x, kube-prometheus-stack picks up prometheus-node-exporter 4.x
                # This changes the selector labels on the daemonset, which is an immutable field, so we remove
                # the daemonset with the old labels before upgrading
                # https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#from-39x-to-40x
                kubectl delete daemonset \
                  -l release=kube-prometheus-stack,app=prometheus-node-exporter \
                  -n {{ .Values.monitoring.kubePrometheusStack.release.namespace }}

                # With the version bump from 2.6.3 to 2.6.4, loki-stack picks up an updated promtail that
                # changes the selector labels on the daemonset, which is an immutable field
                # So we remove the daemonset with the old labels before upgrading
                kubectl delete daemonset \
                  -l release=loki-stack,app=promtail \
                  -n {{ .Values.monitoring.lokiStack.release.namespace }}
          env:
            - name: KUBECONFIG
              value: /etc/kubernetes/config
          resources: {{ toYaml .Values.hooks.resources | nindent 12 }}
          volumeMounts:
            - name: etc-kubernetes
              mountPath: /etc/kubernetes
              readOnly: true
      hostNetwork: {{ .Values.hooks.hostNetwork }}
      {{- with .Values.hooks.nodeSelector }}
      nodeSelector: {{ toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.hooks.affinity }}
      affinity: {{ toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.hooks.tolerations }}
      tolerations: {{ toYaml . | nindent 8 }}
      {{- end }}
      volumes:
        - name: etc-kubernetes
          secret:
            secretName: {{ include "cluster-addons.componentName" (list . "kubeconfig") }}
            optional: true
            items:
              - key: value
                path: config
{{- end }}
